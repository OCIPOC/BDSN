{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cricket - Pipelines in Apache Spark",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praxis-QR/BDSN/blob/main/Cricket_Pipelines_in_Apache_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKSccZgVu7J8"
      },
      "source": [
        "![CC-BY-SA](https://licensebuttons.net/l/by-sa/3.0/88x31.png)<br>\n",
        "<hr>\n",
        "\n",
        "![alt text](http://1.bp.blogspot.com/-nqAGzznZQNo/UwS8rxjfXeI/AAAAAAAABTA/nunmRLowpps/s1600/PraxisLogo.gif)<br>\n",
        "[Data Science Program](http://praxis.ac.in/Programs/business-analytics/)\n",
        "\n",
        "<hr>\n",
        "\n",
        "[Prithwis Mukerjee](http://www.yantrajaal.com)<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR-6MzHBr88H"
      },
      "source": [
        "#Machine Learning Pipelines - A Quick Introduction using PySpark<br>\n",
        "https://www.analyticsvidhya.com/blog/2019/11/build-machine-learning-pipelines-pyspark/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqGXJ8DIxzWN"
      },
      "source": [
        "# Initialise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOr826zIx3Kh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a9d7a11-dd8c-44a7-89f8-9a64b62ef633"
      },
      "source": [
        "!apt-get update > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "#\n",
        "# if the current version of Spark is not used, there may be errors\n",
        "# check here for current versions http://apache.osuosl.org/spark\n",
        "#\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "#!pip install -q findspark\n",
        "!pip install -q pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.4 MB 36 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 54.1 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K_l58UKyQ65"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3hdu86YyVEW"
      },
      "source": [
        "#import findspark\n",
        "#findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "#sc"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPnrUzlSsLN2"
      },
      "source": [
        "# Load Data <br>\n",
        "Data is available at https://drive.google.com/file/d/1wUcLfGZ2HxvJNstZwM1e2CeNDTrJLy9b/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW476lcv5SGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a79f7d2c-ddce-45ca-b4db-e83333c25b90"
      },
      "source": [
        "# Data from Github\n",
        "!wget -q https://raw.githubusercontent.com/Praxis-QR/BDSN/main/Documents/ind-ban-comment.csv\n",
        "!ls -al"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 223604\n",
            "drwxr-xr-x  1 root root      4096 Mar 23 06:31 .\n",
            "drwxr-xr-x  1 root root      4096 Mar 23 06:25 ..\n",
            "drwxr-xr-x  1 root root      4096 Mar  9 14:47 .config\n",
            "-rw-r--r--  1 root root    113629 Mar 23 06:31 ind-ban-comment.csv\n",
            "drwxr-xr-x  1 root root      4096 Mar  9 14:48 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 May 24  2021 spark-3.1.2-bin-hadoop3.2\n",
            "-rw-r--r--  1 root root 228834641 May 24  2021 spark-3.1.2-bin-hadoop3.2.tgz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxj3uWAZye6P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e9a541a-ddca-4547-c9f5-814c3b74a0bb"
      },
      "source": [
        "# read a csv file\n",
        "my_data = spark.read.csv('ind-ban-comment.csv',header=True)\n",
        "\n",
        "# see the default schema of the dataframe\n",
        "my_data.printSchema()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Batsman: string (nullable = true)\n",
            " |-- Batsman_Name: string (nullable = true)\n",
            " |-- Bowler: string (nullable = true)\n",
            " |-- Bowler_Name: string (nullable = true)\n",
            " |-- Commentary: string (nullable = true)\n",
            " |-- Detail: string (nullable = true)\n",
            " |-- Dismissed: string (nullable = true)\n",
            " |-- Id: string (nullable = true)\n",
            " |-- Isball: string (nullable = true)\n",
            " |-- Isboundary: string (nullable = true)\n",
            " |-- Iswicket: string (nullable = true)\n",
            " |-- Over: string (nullable = true)\n",
            " |-- Runs: string (nullable = true)\n",
            " |-- Timestamp: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.types as tp\n",
        "\n",
        "# define the schema\n",
        "my_schema = tp.StructType([\n",
        "    tp.StructField(name= 'Batsman',      dataType= tp.IntegerType(),   nullable= True),\n",
        "    tp.StructField(name= 'Batsman_Name', dataType= tp.StringType(),    nullable= True),\n",
        "    tp.StructField(name= 'Bowler',       dataType= tp.IntegerType(),   nullable= True),\n",
        "    tp.StructField(name= 'Bowler_Name',  dataType= tp.StringType(),    nullable= True),\n",
        "    tp.StructField(name= 'Commentary',   dataType= tp.StringType(),    nullable= True),\n",
        "    tp.StructField(name= 'Detail',       dataType= tp.StringType(),    nullable= True),\n",
        "    tp.StructField(name= 'Dismissed',    dataType= tp.IntegerType(),   nullable= True),\n",
        "    tp.StructField(name= 'Id',           dataType= tp.IntegerType(),   nullable= True),\n",
        "    tp.StructField(name= 'Isball',       dataType= tp.BooleanType(),   nullable= True),\n",
        "    tp.StructField(name= 'Isboundary',   dataType= tp.IntegerType(),   nullable= True),  # changed\n",
        "    tp.StructField(name= 'Iswicket',     dataType= tp.IntegerType(),   nullable= True),  # changed\n",
        "    tp.StructField(name= 'Over',         dataType= tp.DoubleType(),    nullable= True),\n",
        "    tp.StructField(name= 'Runs',         dataType= tp.IntegerType(),   nullable= True),\n",
        "    tp.StructField(name= 'Timestamp',    dataType= tp.TimestampType(), nullable= True)    \n",
        "])\n",
        "\n",
        "# read the data again with the defined schema\n",
        "my_data = spark.read.csv('ind-ban-comment.csv',schema= my_schema,header= True)\n",
        "\n",
        "# print the schema\n",
        "my_data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmOIeATNhVCZ",
        "outputId": "ccfe3078-5973-42ee-d61f-3fbf1bed8b41"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Batsman: integer (nullable = true)\n",
            " |-- Batsman_Name: string (nullable = true)\n",
            " |-- Bowler: integer (nullable = true)\n",
            " |-- Bowler_Name: string (nullable = true)\n",
            " |-- Commentary: string (nullable = true)\n",
            " |-- Detail: string (nullable = true)\n",
            " |-- Dismissed: integer (nullable = true)\n",
            " |-- Id: integer (nullable = true)\n",
            " |-- Isball: boolean (nullable = true)\n",
            " |-- Isboundary: integer (nullable = true)\n",
            " |-- Iswicket: integer (nullable = true)\n",
            " |-- Over: double (nullable = true)\n",
            " |-- Runs: integer (nullable = true)\n",
            " |-- Timestamp: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the columns that are not required\n",
        "my_data = my_data.drop(*['Batsman', 'Bowler', 'Id'])\n",
        "my_data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n7me4LchqBc",
        "outputId": "babf848d-4f25-4d87-8a4b-bf98f30b5f96"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Batsman_Name',\n",
              " 'Bowler_Name',\n",
              " 'Commentary',\n",
              " 'Detail',\n",
              " 'Dismissed',\n",
              " 'Isball',\n",
              " 'Isboundary',\n",
              " 'Iswicket',\n",
              " 'Over',\n",
              " 'Runs',\n",
              " 'Timestamp']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Examine Data"
      ],
      "metadata": {
        "id": "_ZGerTYID3BG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the dimensions of the data\n",
        "(my_data.count() , len(my_data.columns))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMz-oxMND5bL",
        "outputId": "b17652e2-e64a-400b-ec1f-7c375682e0bb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(605, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the summary of the numerical columns\n",
        "my_data.select('Isball', 'Isboundary', 'Runs').describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_qD319Nh92y",
        "outputId": "176de859-4c64-45fa-eeb9-23eaab43d358"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------------------+\n",
            "|summary|Isboundary|              Runs|\n",
            "+-------+----------+------------------+\n",
            "|  count|        67|               605|\n",
            "|   mean|       1.0|0.9917355371900827|\n",
            "| stddev|       0.0| 1.342725481259329|\n",
            "|    min|         1|                 0|\n",
            "|    max|         1|                 6|\n",
            "+-------+----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import sql function pyspark\n",
        "import pyspark.sql.functions as f\n",
        "\n",
        "# null values in each column\n",
        "data_agg = my_data.agg(*[f.count(f.when(f.isnull(c), c)).alias(c) for c in my_data.columns])\n",
        "data_agg.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDFJWZHxiGcg",
        "outputId": "11455d6f-6cb6-4ea1-b546-9171e0baa6d1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+----------+------+---------+------+----------+--------+----+----+---------+\n",
            "|Batsman_Name|Bowler_Name|Commentary|Detail|Dismissed|Isball|Isboundary|Iswicket|Over|Runs|Timestamp|\n",
            "+------------+-----------+----------+------+---------+------+----------+--------+----+----+---------+\n",
            "|           0|          0|         0|   565|      586|     0|       538|     586|   0|   0|        0|\n",
            "+------------+-----------+----------+------+---------+------+----------+--------+----+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# value counts of Batsman_Name column\n",
        "my_data.groupBy('Batsman_Name').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVgLcnzwEEhx",
        "outputId": "ecefb762-6ac8-45af-f848-c4298ef7e71d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----+\n",
            "|      Batsman_Name|count|\n",
            "+------------------+-----+\n",
            "|     Soumya Sarkar|   39|\n",
            "|  Mashrafe Mortaza|    5|\n",
            "|   Shakib Al Hasan|   75|\n",
            "|   Mushfiqur Rahim|   23|\n",
            "|Mohammad Saifuddin|   42|\n",
            "|         Liton Das|   24|\n",
            "|      Rishabh Pant|   43|\n",
            "|    Mohammed Shami|    2|\n",
            "|       Tamim Iqbal|   31|\n",
            "|     Hardik Pandya|    2|\n",
            "|          KL Rahul|   93|\n",
            "| Bhuvneshwar Kumar|    4|\n",
            "|     Rubel Hossain|   11|\n",
            "|      Rohit Sharma|   94|\n",
            "|    Dinesh Karthik|    9|\n",
            "|       Virat Kohli|   27|\n",
            "|          MS Dhoni|   33|\n",
            "|     Sabbir Rahman|   40|\n",
            "|  Mosaddek Hossain|    7|\n",
            "| Mustafizur Rahman|    1|\n",
            "+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding"
      ],
      "metadata": {
        "id": "iDEY9chukElW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "\n",
        "# create object of StringIndexer class and specify input and output column\n",
        "SI_batsman = StringIndexer(inputCol='Batsman_Name',outputCol='Batsman_Index')\n",
        "SI_bowler = StringIndexer(inputCol='Bowler_Name',outputCol='Bowler_Index')\n",
        "\n",
        "# transform the data\n",
        "my_data = SI_batsman.fit(my_data).transform(my_data)\n",
        "my_data = SI_bowler.fit(my_data).transform(my_data)\n",
        "\n",
        "# view the transformed data\n",
        "my_data.select('Batsman_Name', 'Batsman_Index', 'Bowler_Name', 'Bowler_Index').show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tZv6v2AkGa6",
        "outputId": "f86cf9c2-f98c-401d-8450-05b2bfe1502d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------+------------------+------------+\n",
            "|     Batsman_Name|Batsman_Index|       Bowler_Name|Bowler_Index|\n",
            "+-----------------+-------------+------------------+------------+\n",
            "|   Mohammed Shami|         18.0| Mustafizur Rahman|         0.0|\n",
            "|Bhuvneshwar Kumar|         16.0| Mustafizur Rahman|         0.0|\n",
            "|   Mohammed Shami|         18.0| Mustafizur Rahman|         0.0|\n",
            "|Bhuvneshwar Kumar|         16.0| Mustafizur Rahman|         0.0|\n",
            "|         MS Dhoni|          7.0| Mustafizur Rahman|         0.0|\n",
            "|         MS Dhoni|          7.0| Mustafizur Rahman|         0.0|\n",
            "|         MS Dhoni|          7.0| Mustafizur Rahman|         0.0|\n",
            "|         MS Dhoni|          7.0|Mohammad Saifuddin|         8.0|\n",
            "|         MS Dhoni|          7.0|Mohammad Saifuddin|         8.0|\n",
            "|         MS Dhoni|          7.0|Mohammad Saifuddin|         8.0|\n",
            "+-----------------+-------------+------------------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create object and specify input and output column\n",
        "OHE = OneHotEncoder(inputCols=['Batsman_Index', 'Bowler_Index'],outputCols=['Batsman_OHE', 'Bowler_OHE'])\n",
        "\n",
        "# transform the data\n",
        "my_data = OHE.fit(my_data).transform(my_data)\n",
        "\n",
        "# view and transform the data\n",
        "my_data.select('Batsman_Name', 'Batsman_Index', 'Batsman_OHE', 'Bowler_Name', 'Bowler_Index', 'Bowler_OHE').show(10)"
      ],
      "metadata": {
        "id": "1m6kX4-BTIip",
        "outputId": "7f68ec94-3758-4b3b-daff-4e1585167c9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------+---------------+------------------+------------+--------------+\n",
            "|     Batsman_Name|Batsman_Index|    Batsman_OHE|       Bowler_Name|Bowler_Index|    Bowler_OHE|\n",
            "+-----------------+-------------+---------------+------------------+------------+--------------+\n",
            "|   Mohammed Shami|         18.0|(19,[18],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
            "|Bhuvneshwar Kumar|         16.0|(19,[16],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
            "|   Mohammed Shami|         18.0|(19,[18],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
            "|Bhuvneshwar Kumar|         16.0|(19,[16],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
            "|         MS Dhoni|          7.0| (19,[7],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
            "|         MS Dhoni|          7.0| (19,[7],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
            "|         MS Dhoni|          7.0| (19,[7],[1.0])| Mustafizur Rahman|         0.0|(11,[0],[1.0])|\n",
            "|         MS Dhoni|          7.0| (19,[7],[1.0])|Mohammad Saifuddin|         8.0|(11,[8],[1.0])|\n",
            "|         MS Dhoni|          7.0| (19,[7],[1.0])|Mohammad Saifuddin|         8.0|(11,[8],[1.0])|\n",
            "|         MS Dhoni|          7.0| (19,[7],[1.0])|Mohammad Saifuddin|         8.0|(11,[8],[1.0])|\n",
            "+-----------------+-------------+---------------+------------------+------------+--------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# specify the input and output columns of the vector assembler\n",
        "assembler = VectorAssembler(inputCols=['Isboundary',\n",
        "                                       'Iswicket',\n",
        "                                       'Over',\n",
        "                                       'Runs',\n",
        "                                       'Batsman_Index',\n",
        "                                       'Bowler_Index',\n",
        "                                       'Batsman_OHE',\n",
        "                                       'Bowler_OHE'],\n",
        "                           outputCol='vector')\n",
        "\n",
        "# fill the null values\n",
        "my_data = my_data.fillna(0)\n",
        "\n",
        "# transform the data\n",
        "final_data = assembler.transform(my_data)\n",
        "\n",
        "# view the transformed vector\n",
        "final_data.select('vector').show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr2QyH5SELI4",
        "outputId": "db3793a1-0ae9-4bd8-f191-87995b7139cc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|              vector|\n",
            "+--------------------+\n",
            "|(36,[1,2,4,24,25]...|\n",
            "|(36,[1,2,3,4,22,2...|\n",
            "|(36,[2,3,4,24,25]...|\n",
            "|(36,[2,3,4,22,25]...|\n",
            "|(36,[1,2,4,13,25]...|\n",
            "|(36,[2,4,13,25],[...|\n",
            "|(36,[2,4,13,25],[...|\n",
            "|(36,[2,3,4,5,13,3...|\n",
            "|(36,[0,2,3,4,5,13...|\n",
            "|(36,[2,4,5,13,33]...|\n",
            "|(36,[2,4,5,13,33]...|\n",
            "|(36,[0,2,3,4,5,13...|\n",
            "|(36,[2,3,4,5,13,3...|\n",
            "|(36,[2,4,22,25],[...|\n",
            "|(36,[2,3,4,13,25]...|\n",
            "|(36,[2,4,13,25],[...|\n",
            "|(36,[2,3,4,22,25]...|\n",
            "|(36,[1,2,4,19,25]...|\n",
            "|(36,[2,3,4,13,25]...|\n",
            "|(36,[2,3,4,5,13,3...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Diversion into Pipelines"
      ],
      "metadata": {
        "id": "Zjc9JqUlTVG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# create a sample dataframe\n",
        "sample_df = spark.createDataFrame([\n",
        "    (1, 'L101', 'R'),\n",
        "    (2, 'L201', 'C'),\n",
        "    (3, 'D111', 'R'),\n",
        "    (4, 'F210', 'R'),\n",
        "    (5, 'D110', 'C')\n",
        "], ['id', 'category_1', 'category_2'])\n",
        "\n",
        "sample_df.show()"
      ],
      "metadata": {
        "id": "iYbvC7q-TYXn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9696a778-d93b-4f47-8ff0-7772298e9aac"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+----------+\n",
            "| id|category_1|category_2|\n",
            "+---+----------+----------+\n",
            "|  1|      L101|         R|\n",
            "|  2|      L201|         C|\n",
            "|  3|      D111|         R|\n",
            "|  4|      F210|         R|\n",
            "|  5|      D110|         C|\n",
            "+---+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define stage 1 : transform the column category_1 to numeric\n",
        "stage_1 = StringIndexer(inputCol= 'category_1', outputCol= 'category_1_index')\n",
        "# define stage 2 : transform the column category_2 to numeric\n",
        "stage_2 = StringIndexer(inputCol= 'category_2', outputCol= 'category_2_index')\n",
        "# define stage 3 : one hot encode the numeric category_2 column\n",
        "stage_3 = OneHotEncoder(inputCols=['category_2_index'], outputCols=['category_2_OHE'])\n",
        "\n",
        "# setup the pipeline\n",
        "pipeline = Pipeline(stages=[stage_1, stage_2, stage_3])\n",
        "\n",
        "# fit the pipeline model and transform the data as defined\n",
        "pipeline_model = pipeline.fit(sample_df)\n",
        "sample_df_updated = pipeline_model.transform(sample_df)\n",
        "\n",
        "# view the transformed data\n",
        "sample_df_updated.show()"
      ],
      "metadata": {
        "id": "24YEJ_KeTlCf",
        "outputId": "569db323-ad71-4375-b504-9581f8e96067",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+----------+----------------+----------------+--------------+\n",
            "| id|category_1|category_2|category_1_index|category_2_index|category_2_OHE|\n",
            "+---+----------+----------+----------------+----------------+--------------+\n",
            "|  1|      L101|         R|             3.0|             0.0| (1,[0],[1.0])|\n",
            "|  2|      L201|         C|             4.0|             1.0|     (1,[],[])|\n",
            "|  3|      D111|         R|             1.0|             0.0| (1,[0],[1.0])|\n",
            "|  4|      F210|         R|             2.0|             0.0| (1,[0],[1.0])|\n",
            "|  5|      D110|         C|             0.0|             1.0|     (1,[],[])|\n",
            "+---+----------+----------+----------------+----------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# create a sample dataframe with 4 features and 1 label column\n",
        "sample_data_train = spark.createDataFrame([\n",
        "    (2.0, 'A', 'S10', 40, 1.0),\n",
        "    (1.0, 'X', 'E10', 25, 1.0),\n",
        "    (4.0, 'X', 'S20', 10, 0.0),\n",
        "    (3.0, 'Z', 'S10', 20, 0.0),\n",
        "    (4.0, 'A', 'E10', 30, 1.0),\n",
        "    (2.0, 'Z', 'S10', 40, 0.0),\n",
        "    (5.0, 'X', 'D10', 10, 1.0),\n",
        "], ['feature_1', 'feature_2', 'feature_3', 'feature_4', 'label'])\n",
        "\n",
        "# view the data\n",
        "sample_data_train.show()"
      ],
      "metadata": {
        "id": "jBcgyFC8TuE5",
        "outputId": "465efdb6-ad96-419f-e81d-01a9ae180bee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+---------+---------+-----+\n",
            "|feature_1|feature_2|feature_3|feature_4|label|\n",
            "+---------+---------+---------+---------+-----+\n",
            "|      2.0|        A|      S10|       40|  1.0|\n",
            "|      1.0|        X|      E10|       25|  1.0|\n",
            "|      4.0|        X|      S20|       10|  0.0|\n",
            "|      3.0|        Z|      S10|       20|  0.0|\n",
            "|      4.0|        A|      E10|       30|  1.0|\n",
            "|      2.0|        Z|      S10|       40|  0.0|\n",
            "|      5.0|        X|      D10|       10|  1.0|\n",
            "+---------+---------+---------+---------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define stage 1: transform the column feature_2 to numeric\n",
        "stage_1 = StringIndexer(inputCol= 'feature_2', outputCol= 'feature_2_index')\n",
        "# define stage 2: transform the column feature_3 to numeric\n",
        "stage_2 = StringIndexer(inputCol= 'feature_3', outputCol= 'feature_3_index')\n",
        "# define stage 3: one hot encode the numeric versions of feature 2 and 3 generated from stage 1 and stage 2\n",
        "stage_3 = OneHotEncoder(inputCols=[stage_1.getOutputCol(), stage_2.getOutputCol()], \n",
        "                                 outputCols= ['feature_2_encoded', 'feature_3_encoded'])\n",
        "# define stage 4: create a vector of all the features required to train the logistic regression model \n",
        "stage_4 = VectorAssembler(inputCols=['feature_1', 'feature_2_encoded', 'feature_3_encoded', 'feature_4'],\n",
        "                          outputCol='features')\n",
        "# define stage 5: logistic regression model                          \n",
        "stage_5 = LogisticRegression(featuresCol='features',labelCol='label')\n",
        "\n",
        "# setup the pipeline\n",
        "regression_pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, stage_4, stage_5])\n",
        "\n",
        "# fit the pipeline for the trainind data\n",
        "model = regression_pipeline.fit(sample_data_train)\n",
        "# transform the data\n",
        "sample_data_train = model.transform(sample_data_train)\n",
        "\n",
        "# view some of the columns generated\n",
        "sample_data_train.select('features', 'label', 'rawPrediction', 'probability', 'prediction').show()\n"
      ],
      "metadata": {
        "id": "XdlBdZ-lUOib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5369689a-502d-4d27-bbc0-2b25d5a08a3d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+--------------------+--------------------+----------+\n",
            "|            features|label|       rawPrediction|         probability|prediction|\n",
            "+--------------------+-----+--------------------+--------------------+----------+\n",
            "|[2.0,0.0,1.0,1.0,...|  1.0|[-18.956871848872...|[5.84972108438303...|       1.0|\n",
            "|[1.0,1.0,0.0,0.0,...|  1.0|[-20.158269476977...|[1.75944137519401...|       1.0|\n",
            "|(7,[0,1,6],[4.0,1...|  0.0|[18.0148602858559...|[0.99999998499466...|       0.0|\n",
            "|(7,[0,3,6],[3.0,1...|  0.0|[24.5051237560208...|[0.99999999997721...|       0.0|\n",
            "|[4.0,0.0,1.0,0.0,...|  1.0|[-50.288624611180...|[1.44519958724618...|       1.0|\n",
            "|(7,[0,3,6],[2.0,1...|  0.0|[18.3280841853904...|[0.99999998902980...|       0.0|\n",
            "|[5.0,1.0,0.0,0.0,...|  1.0|[-17.986823547340...|[1.54319845459404...|       1.0|\n",
            "+--------------------+-----+--------------------+--------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a sample data without the labels\n",
        "sample_data_test = spark.createDataFrame([\n",
        "    (3.0, 'Z', 'S10', 40),\n",
        "    (1.0, 'X', 'E10', 20),\n",
        "    (4.0, 'A', 'S20', 10),\n",
        "    (3.0, 'A', 'S10', 20),\n",
        "    (4.0, 'X', 'D10', 30),\n",
        "    (1.0, 'Z', 'E10', 20),\n",
        "    (4.0, 'A', 'S10', 30),\n",
        "], ['feature_1', 'feature_2', 'feature_3', 'feature_4'])\n",
        "\n",
        "# transform the data using the pipeline\n",
        "sample_data_test = model.transform(sample_data_test)\n",
        "\n",
        "# see the prediction on the test data\n",
        "sample_data_test.select('features', 'rawPrediction', 'probability', 'prediction').show()"
      ],
      "metadata": {
        "id": "gndZgbhIUe0b",
        "outputId": "0af9a6ce-4e7b-428f-da33-634cfd0f56c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+----------+\n",
            "|            features|       rawPrediction|         probability|prediction|\n",
            "+--------------------+--------------------+--------------------+----------+\n",
            "|(7,[0,3,6],[3.0,1...|[21.9361235191356...|[0.99999999970265...|       0.0|\n",
            "|[1.0,1.0,0.0,0.0,...|[-19.516019417755...|[3.34426325212805...|       1.0|\n",
            "|(7,[0,2,6],[4.0,1...|[-22.297362790362...|[2.07194574533595...|       1.0|\n",
            "|[3.0,0.0,1.0,1.0,...|[-12.779832278242...|[2.81700837725024...|       1.0|\n",
            "|[4.0,1.0,0.0,0.0,...|[-24.163863117971...|[3.20455394170289...|       1.0|\n",
            "|(7,[0,4,6],[1.0,1...|[-22.543286459710...|[1.62022409523206...|       1.0|\n",
            "|[4.0,0.0,1.0,1.0,...|[-10.456293062939...|[2.87658445082394...|       1.0|\n",
            "+--------------------+--------------------+--------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE1gjD5yNfrO"
      },
      "source": [
        "#Chronobooks <br>\n",
        "![alt text](https://1.bp.blogspot.com/-lTiYBkU2qbU/X1er__fvnkI/AAAAAAAAjtE/GhDR3OEGJr4NG43fZPodrQD5kbxtnKebgCLcBGAsYHQ/s600/Footer2020-600x200.png)<hr>\n",
        "Chronotantra and Chronoyantra are two science fiction novels that explore the collapse of human civilisation on Earth and then its rebirth and reincarnation both on Earth as well as on the distant worlds of Mars, Titan and Enceladus. But is it the human civilisation that is being reborn? Or is it some other sentience that is revealing itself. \n",
        "If you have an interest in AI and found this material useful, you may consider buying these novels, in paperback or kindle, from [http://bit.ly/chronobooks](http://bit.ly/chronobooks)"
      ]
    }
  ]
}